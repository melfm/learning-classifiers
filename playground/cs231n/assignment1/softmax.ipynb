{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.344102\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.579078 analytic: -0.579078, relative error: 1.482723e-08\n",
      "numerical: -0.059455 analytic: -0.059456, relative error: 6.028124e-07\n",
      "numerical: 0.656415 analytic: 0.656414, relative error: 8.703304e-08\n",
      "numerical: 2.249778 analytic: 2.249778, relative error: 1.468011e-08\n",
      "numerical: 1.007098 analytic: 1.007098, relative error: 1.163315e-08\n",
      "numerical: 0.998936 analytic: 0.998936, relative error: 4.570784e-09\n",
      "numerical: 0.379978 analytic: 0.379978, relative error: 7.832787e-08\n",
      "numerical: 1.041936 analytic: 1.041936, relative error: 4.253971e-10\n",
      "numerical: -0.151628 analytic: -0.151627, relative error: 2.623367e-07\n",
      "numerical: -0.869884 analytic: -0.869884, relative error: 9.346126e-09\n",
      "numerical: -5.924539 analytic: -5.926556, relative error: 1.701352e-04\n",
      "numerical: -0.609678 analytic: -0.611081, relative error: 1.149865e-03\n",
      "numerical: -0.353941 analytic: -0.354659, relative error: 1.013050e-03\n",
      "numerical: 0.119497 analytic: 0.116094, relative error: 1.444610e-02\n",
      "numerical: -0.494337 analytic: -0.502379, relative error: 8.067613e-03\n",
      "numerical: 3.841884 analytic: 3.829597, relative error: 1.601653e-03\n",
      "numerical: -3.285258 analytic: -3.288999, relative error: 5.690443e-04\n",
      "numerical: -3.182127 analytic: -3.186216, relative error: 6.421385e-04\n",
      "numerical: 0.124516 analytic: 0.132991, relative error: 3.291262e-02\n",
      "numerical: 0.484484 analytic: 0.478800, relative error: 5.899935e-03\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.344102e+00 computed in 0.169868s\n",
      "vectorized loss: 2.344102e+00 computed in 0.008904s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying learning_rate:  1e-07  and regularization strength:  25000.0\n",
      "iteration 0 / 2000: loss 763.892429\n",
      "iteration 100 / 2000: loss 461.306520\n",
      "iteration 200 / 2000: loss 280.253949\n",
      "iteration 300 / 2000: loss 170.299310\n",
      "iteration 400 / 2000: loss 103.710416\n",
      "iteration 500 / 2000: loss 63.668501\n",
      "iteration 600 / 2000: loss 39.280702\n",
      "iteration 700 / 2000: loss 24.638449\n",
      "iteration 800 / 2000: loss 15.708511\n",
      "iteration 900 / 2000: loss 10.424115\n",
      "iteration 1000 / 2000: loss 7.165053\n",
      "iteration 1100 / 2000: loss 5.074362\n",
      "iteration 1200 / 2000: loss 3.926375\n",
      "iteration 1300 / 2000: loss 3.166746\n",
      "iteration 1400 / 2000: loss 2.735560\n",
      "iteration 1500 / 2000: loss 2.591082\n",
      "iteration 1600 / 2000: loss 2.313180\n",
      "iteration 1700 / 2000: loss 2.251243\n",
      "iteration 1800 / 2000: loss 2.204598\n",
      "iteration 1900 / 2000: loss 2.099800\n",
      "That took 6.889719s\n",
      "training accuracy: 0.353531\n",
      "validation accuracy: 0.367000\n",
      "Trying learning_rate:  1e-07  and regularization strength:  31250.0\n",
      "iteration 0 / 2000: loss 958.365042\n",
      "iteration 100 / 2000: loss 511.414948\n",
      "iteration 200 / 2000: loss 273.895430\n",
      "iteration 300 / 2000: loss 147.224819\n",
      "iteration 400 / 2000: loss 79.519263\n",
      "iteration 500 / 2000: loss 43.421895\n",
      "iteration 600 / 2000: loss 24.124959\n",
      "iteration 700 / 2000: loss 13.960767\n",
      "iteration 800 / 2000: loss 8.442127\n",
      "iteration 900 / 2000: loss 5.504558\n",
      "iteration 1000 / 2000: loss 3.953080\n",
      "iteration 1100 / 2000: loss 3.085459\n",
      "iteration 1200 / 2000: loss 2.700055\n",
      "iteration 1300 / 2000: loss 2.403568\n",
      "iteration 1400 / 2000: loss 2.311890\n",
      "iteration 1500 / 2000: loss 2.217943\n",
      "iteration 1600 / 2000: loss 2.133959\n",
      "iteration 1700 / 2000: loss 2.154794\n",
      "iteration 1800 / 2000: loss 2.082278\n",
      "iteration 1900 / 2000: loss 2.154927\n",
      "That took 6.683510s\n",
      "training accuracy: 0.344551\n",
      "validation accuracy: 0.355000\n",
      "Trying learning_rate:  1e-07  and regularization strength:  37500.0\n",
      "iteration 0 / 2000: loss 1162.226150\n",
      "iteration 100 / 2000: loss 547.525730\n",
      "iteration 200 / 2000: loss 258.648644\n",
      "iteration 300 / 2000: loss 122.843895\n",
      "iteration 400 / 2000: loss 58.998898\n",
      "iteration 500 / 2000: loss 28.840052\n",
      "iteration 600 / 2000: loss 14.749614\n",
      "iteration 700 / 2000: loss 8.024357\n",
      "iteration 800 / 2000: loss 4.871630\n",
      "iteration 900 / 2000: loss 3.422773\n",
      "iteration 1000 / 2000: loss 2.757955\n",
      "iteration 1100 / 2000: loss 2.426070\n",
      "iteration 1200 / 2000: loss 2.289036\n",
      "iteration 1300 / 2000: loss 2.240727\n",
      "iteration 1400 / 2000: loss 2.139693\n",
      "iteration 1500 / 2000: loss 2.160703\n",
      "iteration 1600 / 2000: loss 2.142936\n",
      "iteration 1700 / 2000: loss 2.113051\n",
      "iteration 1800 / 2000: loss 2.112766\n",
      "iteration 1900 / 2000: loss 2.133117\n",
      "That took 6.613796s\n",
      "training accuracy: 0.337041\n",
      "validation accuracy: 0.360000\n",
      "Trying learning_rate:  1e-07  and regularization strength:  43750.0\n",
      "iteration 0 / 2000: loss 1327.770505\n",
      "iteration 100 / 2000: loss 551.293399\n",
      "iteration 200 / 2000: loss 230.119057\n",
      "iteration 300 / 2000: loss 96.812910\n",
      "iteration 400 / 2000: loss 41.412733\n",
      "iteration 500 / 2000: loss 18.494297\n",
      "iteration 600 / 2000: loss 8.957231\n",
      "iteration 700 / 2000: loss 4.958787\n",
      "iteration 800 / 2000: loss 3.324414\n",
      "iteration 900 / 2000: loss 2.648436\n",
      "iteration 1000 / 2000: loss 2.381718\n",
      "iteration 1100 / 2000: loss 2.237890\n",
      "iteration 1200 / 2000: loss 2.153169\n",
      "iteration 1300 / 2000: loss 2.205596\n",
      "iteration 1400 / 2000: loss 2.170623\n",
      "iteration 1500 / 2000: loss 2.223717\n",
      "iteration 1600 / 2000: loss 2.236514\n",
      "iteration 1700 / 2000: loss 2.115909\n",
      "iteration 1800 / 2000: loss 2.116929\n",
      "iteration 1900 / 2000: loss 2.169249\n",
      "That took 6.505341s\n",
      "training accuracy: 0.328939\n",
      "validation accuracy: 0.339000\n",
      "Trying learning_rate:  1e-07  and regularization strength:  50000.0\n",
      "iteration 0 / 2000: loss 1540.680236\n",
      "iteration 100 / 2000: loss 564.284742\n",
      "iteration 200 / 2000: loss 207.820092\n",
      "iteration 300 / 2000: loss 77.419300\n",
      "iteration 400 / 2000: loss 29.714629\n",
      "iteration 500 / 2000: loss 12.253842\n",
      "iteration 600 / 2000: loss 5.827297\n",
      "iteration 700 / 2000: loss 3.511072\n",
      "iteration 800 / 2000: loss 2.679627\n",
      "iteration 900 / 2000: loss 2.349874\n",
      "iteration 1000 / 2000: loss 2.159269\n",
      "iteration 1100 / 2000: loss 2.166099\n",
      "iteration 1200 / 2000: loss 2.126186\n",
      "iteration 1300 / 2000: loss 2.202524\n",
      "iteration 1400 / 2000: loss 2.096292\n",
      "iteration 1500 / 2000: loss 2.193140\n",
      "iteration 1600 / 2000: loss 2.210388\n",
      "iteration 1700 / 2000: loss 2.110212\n",
      "iteration 1800 / 2000: loss 2.162622\n",
      "iteration 1900 / 2000: loss 2.121942\n",
      "That took 6.380257s\n",
      "training accuracy: 0.332224\n",
      "validation accuracy: 0.342000\n",
      "Trying learning_rate:  2e-07  and regularization strength:  25000.0\n",
      "iteration 0 / 2000: loss 764.096053\n",
      "iteration 100 / 2000: loss 279.892846\n",
      "iteration 200 / 2000: loss 103.604638\n",
      "iteration 300 / 2000: loss 39.118336\n",
      "iteration 400 / 2000: loss 15.686081\n",
      "iteration 500 / 2000: loss 7.079654\n",
      "iteration 600 / 2000: loss 3.968167\n",
      "iteration 700 / 2000: loss 2.823191\n",
      "iteration 800 / 2000: loss 2.357344\n",
      "iteration 900 / 2000: loss 2.203999\n",
      "iteration 1000 / 2000: loss 2.180304\n",
      "iteration 1100 / 2000: loss 2.109925\n",
      "iteration 1200 / 2000: loss 2.068079\n",
      "iteration 1300 / 2000: loss 2.025643\n",
      "iteration 1400 / 2000: loss 2.168467\n",
      "iteration 1500 / 2000: loss 2.057857\n",
      "iteration 1600 / 2000: loss 2.125271\n",
      "iteration 1700 / 2000: loss 2.126840\n",
      "iteration 1800 / 2000: loss 2.120574\n",
      "iteration 1900 / 2000: loss 2.152900\n",
      "That took 7.163972s\n",
      "training accuracy: 0.346755\n",
      "validation accuracy: 0.369000\n",
      "Trying learning_rate:  2e-07  and regularization strength:  31250.0\n",
      "iteration 0 / 2000: loss 966.136858\n",
      "iteration 100 / 2000: loss 275.753983\n",
      "iteration 200 / 2000: loss 79.848905\n",
      "iteration 300 / 2000: loss 24.183587\n",
      "iteration 400 / 2000: loss 8.449841\n",
      "iteration 500 / 2000: loss 3.881339\n",
      "iteration 600 / 2000: loss 2.672304\n",
      "iteration 700 / 2000: loss 2.257440\n",
      "iteration 800 / 2000: loss 2.204124\n",
      "iteration 900 / 2000: loss 2.113910\n",
      "iteration 1000 / 2000: loss 2.119105\n",
      "iteration 1100 / 2000: loss 2.161382\n",
      "iteration 1200 / 2000: loss 2.126607\n",
      "iteration 1300 / 2000: loss 2.135144\n",
      "iteration 1400 / 2000: loss 2.158911\n",
      "iteration 1500 / 2000: loss 2.154419\n",
      "iteration 1600 / 2000: loss 2.082047\n",
      "iteration 1700 / 2000: loss 2.167312\n",
      "iteration 1800 / 2000: loss 2.132749\n",
      "iteration 1900 / 2000: loss 2.101307\n",
      "That took 6.617779s\n",
      "training accuracy: 0.343245\n",
      "validation accuracy: 0.361000\n",
      "Trying learning_rate:  2e-07  and regularization strength:  37500.0\n",
      "iteration 0 / 2000: loss 1164.658173\n",
      "iteration 100 / 2000: loss 258.710789\n",
      "iteration 200 / 2000: loss 58.722476\n",
      "iteration 300 / 2000: loss 14.755887\n",
      "iteration 400 / 2000: loss 4.918571\n",
      "iteration 500 / 2000: loss 2.733838\n",
      "iteration 600 / 2000: loss 2.218149\n",
      "iteration 700 / 2000: loss 2.154677\n",
      "iteration 800 / 2000: loss 2.102700\n",
      "iteration 900 / 2000: loss 2.171712\n",
      "iteration 1000 / 2000: loss 2.143329\n",
      "iteration 1100 / 2000: loss 2.215045\n",
      "iteration 1200 / 2000: loss 2.186699\n",
      "iteration 1300 / 2000: loss 2.156455\n",
      "iteration 1400 / 2000: loss 2.197124\n",
      "iteration 1500 / 2000: loss 2.139969\n",
      "iteration 1600 / 2000: loss 2.152934\n",
      "iteration 1700 / 2000: loss 2.145321\n",
      "iteration 1800 / 2000: loss 2.164452\n",
      "iteration 1900 / 2000: loss 2.125982\n",
      "That took 7.324349s\n",
      "training accuracy: 0.340878\n",
      "validation accuracy: 0.351000\n",
      "Trying learning_rate:  2e-07  and regularization strength:  43750.0\n",
      "iteration 0 / 2000: loss 1347.824979\n",
      "iteration 100 / 2000: loss 232.630115\n",
      "iteration 200 / 2000: loss 41.670850\n",
      "iteration 300 / 2000: loss 8.954911\n",
      "iteration 400 / 2000: loss 3.338174\n",
      "iteration 500 / 2000: loss 2.369267\n",
      "iteration 600 / 2000: loss 2.207800\n",
      "iteration 700 / 2000: loss 2.244254\n",
      "iteration 800 / 2000: loss 2.204608\n",
      "iteration 900 / 2000: loss 2.154350\n",
      "iteration 1000 / 2000: loss 2.195534\n",
      "iteration 1100 / 2000: loss 2.204273\n",
      "iteration 1200 / 2000: loss 2.215457\n",
      "iteration 1300 / 2000: loss 2.099489\n",
      "iteration 1400 / 2000: loss 2.113524\n",
      "iteration 1500 / 2000: loss 2.123769\n",
      "iteration 1600 / 2000: loss 2.179806\n",
      "iteration 1700 / 2000: loss 2.227550\n",
      "iteration 1800 / 2000: loss 2.143743\n",
      "iteration 1900 / 2000: loss 2.133978\n",
      "That took 6.857408s\n",
      "training accuracy: 0.326571\n",
      "validation accuracy: 0.340000\n",
      "Trying learning_rate:  2e-07  and regularization strength:  50000.0\n",
      "iteration 0 / 2000: loss 1550.269377\n",
      "iteration 100 / 2000: loss 208.150272\n",
      "iteration 200 / 2000: loss 29.573188\n",
      "iteration 300 / 2000: loss 5.795185\n",
      "iteration 400 / 2000: loss 2.611760\n",
      "iteration 500 / 2000: loss 2.239939\n",
      "iteration 600 / 2000: loss 2.173159\n",
      "iteration 700 / 2000: loss 2.138317\n",
      "iteration 800 / 2000: loss 2.134734\n",
      "iteration 900 / 2000: loss 2.160214\n",
      "iteration 1000 / 2000: loss 2.206316\n",
      "iteration 1100 / 2000: loss 2.126322\n",
      "iteration 1200 / 2000: loss 2.180385\n",
      "iteration 1300 / 2000: loss 2.168281\n",
      "iteration 1400 / 2000: loss 2.135538\n",
      "iteration 1500 / 2000: loss 2.182890\n",
      "iteration 1600 / 2000: loss 2.192492\n",
      "iteration 1700 / 2000: loss 2.156794\n",
      "iteration 1800 / 2000: loss 2.160185\n",
      "iteration 1900 / 2000: loss 2.168326\n",
      "That took 6.621184s\n",
      "training accuracy: 0.327204\n",
      "validation accuracy: 0.338000\n",
      "Trying learning_rate:  3e-07  and regularization strength:  25000.0\n",
      "iteration 0 / 2000: loss 760.414520\n",
      "iteration 100 / 2000: loss 168.820364\n",
      "iteration 200 / 2000: loss 38.850378\n",
      "iteration 300 / 2000: loss 10.234275\n",
      "iteration 400 / 2000: loss 3.855218\n",
      "iteration 500 / 2000: loss 2.564128\n",
      "iteration 600 / 2000: loss 2.232745\n",
      "iteration 700 / 2000: loss 2.196607\n",
      "iteration 800 / 2000: loss 2.183598\n",
      "iteration 900 / 2000: loss 2.192526\n",
      "iteration 1000 / 2000: loss 2.036585\n",
      "iteration 1100 / 2000: loss 2.111550\n",
      "iteration 1200 / 2000: loss 2.130727\n",
      "iteration 1300 / 2000: loss 2.128113\n",
      "iteration 1400 / 2000: loss 2.094817\n",
      "iteration 1500 / 2000: loss 2.109741\n",
      "iteration 1600 / 2000: loss 2.112911\n",
      "iteration 1700 / 2000: loss 2.181211\n",
      "iteration 1800 / 2000: loss 2.103497\n",
      "iteration 1900 / 2000: loss 2.118395\n",
      "That took 6.614291s\n",
      "training accuracy: 0.353143\n",
      "validation accuracy: 0.351000\n",
      "Trying learning_rate:  3e-07  and regularization strength:  31250.0\n",
      "iteration 0 / 2000: loss 970.268512\n",
      "iteration 100 / 2000: loss 147.932395\n",
      "iteration 200 / 2000: loss 24.094128\n",
      "iteration 300 / 2000: loss 5.440219\n",
      "iteration 400 / 2000: loss 2.619879\n",
      "iteration 500 / 2000: loss 2.174182\n",
      "iteration 600 / 2000: loss 2.086821\n",
      "iteration 700 / 2000: loss 2.156825\n",
      "iteration 800 / 2000: loss 2.150915\n",
      "iteration 900 / 2000: loss 2.155955\n",
      "iteration 1000 / 2000: loss 2.190191\n",
      "iteration 1100 / 2000: loss 2.150717\n",
      "iteration 1200 / 2000: loss 2.171735\n",
      "iteration 1300 / 2000: loss 2.071452\n",
      "iteration 1400 / 2000: loss 2.178157\n",
      "iteration 1500 / 2000: loss 2.128062\n",
      "iteration 1600 / 2000: loss 2.123110\n",
      "iteration 1700 / 2000: loss 2.118552\n",
      "iteration 1800 / 2000: loss 2.123764\n",
      "iteration 1900 / 2000: loss 2.100790\n",
      "That took 6.622360s\n",
      "training accuracy: 0.340122\n",
      "validation accuracy: 0.355000\n",
      "Trying learning_rate:  3e-07  and regularization strength:  37500.0\n",
      "iteration 0 / 2000: loss 1155.286702\n",
      "iteration 100 / 2000: loss 121.003198\n",
      "iteration 200 / 2000: loss 14.491433\n",
      "iteration 300 / 2000: loss 3.448800\n",
      "iteration 400 / 2000: loss 2.314685\n",
      "iteration 500 / 2000: loss 2.095665\n",
      "iteration 600 / 2000: loss 2.141677\n",
      "iteration 700 / 2000: loss 2.260267\n",
      "iteration 800 / 2000: loss 2.132021\n",
      "iteration 900 / 2000: loss 2.236128\n",
      "iteration 1000 / 2000: loss 2.157810\n",
      "iteration 1100 / 2000: loss 2.158796\n",
      "iteration 1200 / 2000: loss 2.187167\n",
      "iteration 1300 / 2000: loss 2.076905\n",
      "iteration 1400 / 2000: loss 2.093529\n",
      "iteration 1500 / 2000: loss 2.118220\n",
      "iteration 1600 / 2000: loss 2.199487\n",
      "iteration 1700 / 2000: loss 2.123861\n",
      "iteration 1800 / 2000: loss 2.148838\n",
      "iteration 1900 / 2000: loss 2.100411\n",
      "That took 6.461296s\n",
      "training accuracy: 0.342224\n",
      "validation accuracy: 0.348000\n",
      "Trying learning_rate:  3e-07  and regularization strength:  43750.0\n",
      "iteration 0 / 2000: loss 1332.255246\n",
      "iteration 100 / 2000: loss 96.001196\n",
      "iteration 200 / 2000: loss 8.744838\n",
      "iteration 300 / 2000: loss 2.544003\n",
      "iteration 400 / 2000: loss 2.166762\n",
      "iteration 500 / 2000: loss 2.234533\n",
      "iteration 600 / 2000: loss 2.119653\n",
      "iteration 700 / 2000: loss 2.206806\n",
      "iteration 800 / 2000: loss 2.120421\n",
      "iteration 900 / 2000: loss 2.169266\n",
      "iteration 1000 / 2000: loss 2.173059\n",
      "iteration 1100 / 2000: loss 2.150433\n",
      "iteration 1200 / 2000: loss 2.185481\n",
      "iteration 1300 / 2000: loss 2.143836\n",
      "iteration 1400 / 2000: loss 2.238438\n",
      "iteration 1500 / 2000: loss 2.175983\n",
      "iteration 1600 / 2000: loss 2.178152\n",
      "iteration 1700 / 2000: loss 2.184096\n",
      "iteration 1800 / 2000: loss 2.109352\n",
      "iteration 1900 / 2000: loss 2.161695\n",
      "That took 6.432489s\n",
      "training accuracy: 0.328143\n",
      "validation accuracy: 0.337000\n",
      "Trying learning_rate:  3e-07  and regularization strength:  50000.0\n",
      "iteration 0 / 2000: loss 1530.381694\n",
      "iteration 100 / 2000: loss 75.917571\n",
      "iteration 200 / 2000: loss 5.747779\n",
      "iteration 300 / 2000: loss 2.321039\n",
      "iteration 400 / 2000: loss 2.190133\n",
      "iteration 500 / 2000: loss 2.194598\n",
      "iteration 600 / 2000: loss 2.234829\n",
      "iteration 700 / 2000: loss 2.212750\n",
      "iteration 800 / 2000: loss 2.194804\n",
      "iteration 900 / 2000: loss 2.209315\n",
      "iteration 1000 / 2000: loss 2.193149\n",
      "iteration 1100 / 2000: loss 2.128022\n",
      "iteration 1200 / 2000: loss 2.181897\n",
      "iteration 1300 / 2000: loss 2.171960\n",
      "iteration 1400 / 2000: loss 2.228091\n",
      "iteration 1500 / 2000: loss 2.170793\n",
      "iteration 1600 / 2000: loss 2.144625\n",
      "iteration 1700 / 2000: loss 2.171117\n",
      "iteration 1800 / 2000: loss 2.164804\n",
      "iteration 1900 / 2000: loss 2.195816\n",
      "That took 6.437786s\n",
      "training accuracy: 0.327755\n",
      "validation accuracy: 0.340000\n",
      "Trying learning_rate:  4e-07  and regularization strength:  25000.0\n",
      "iteration 0 / 2000: loss 764.362438\n",
      "iteration 100 / 2000: loss 103.071337\n",
      "iteration 200 / 2000: loss 15.560716\n",
      "iteration 300 / 2000: loss 3.836836\n",
      "iteration 400 / 2000: loss 2.341656\n",
      "iteration 500 / 2000: loss 2.194524\n",
      "iteration 600 / 2000: loss 2.107800\n",
      "iteration 700 / 2000: loss 2.139178\n",
      "iteration 800 / 2000: loss 2.109697\n",
      "iteration 900 / 2000: loss 2.134875\n",
      "iteration 1000 / 2000: loss 2.110229\n",
      "iteration 1100 / 2000: loss 2.104257\n",
      "iteration 1200 / 2000: loss 2.101155\n",
      "iteration 1300 / 2000: loss 2.109583\n",
      "iteration 1400 / 2000: loss 2.079602\n",
      "iteration 1500 / 2000: loss 2.164622\n",
      "iteration 1600 / 2000: loss 2.205770\n",
      "iteration 1700 / 2000: loss 2.069840\n",
      "iteration 1800 / 2000: loss 2.151053\n",
      "iteration 1900 / 2000: loss 2.123653\n",
      "That took 6.517300s\n",
      "training accuracy: 0.348122\n",
      "validation accuracy: 0.364000\n",
      "Trying learning_rate:  4e-07  and regularization strength:  31250.0\n",
      "iteration 0 / 2000: loss 960.607095\n",
      "iteration 100 / 2000: loss 78.693088\n",
      "iteration 200 / 2000: loss 8.246187\n",
      "iteration 300 / 2000: loss 2.644823\n",
      "iteration 400 / 2000: loss 2.202031\n",
      "iteration 500 / 2000: loss 2.117740\n",
      "iteration 600 / 2000: loss 2.149760\n",
      "iteration 700 / 2000: loss 2.168064\n",
      "iteration 800 / 2000: loss 2.015547\n",
      "iteration 900 / 2000: loss 2.126935\n",
      "iteration 1000 / 2000: loss 2.135691\n",
      "iteration 1100 / 2000: loss 2.192065\n",
      "iteration 1200 / 2000: loss 2.065507\n",
      "iteration 1300 / 2000: loss 2.147555\n",
      "iteration 1400 / 2000: loss 2.135726\n",
      "iteration 1500 / 2000: loss 2.083964\n",
      "iteration 1600 / 2000: loss 2.104948\n",
      "iteration 1700 / 2000: loss 2.178353\n",
      "iteration 1800 / 2000: loss 2.199553\n",
      "iteration 1900 / 2000: loss 2.150054\n",
      "That took 7.013975s\n",
      "training accuracy: 0.332918\n",
      "validation accuracy: 0.339000\n",
      "Trying learning_rate:  4e-07  and regularization strength:  37500.0\n",
      "iteration 0 / 2000: loss 1161.452686\n",
      "iteration 100 / 2000: loss 57.903182\n",
      "iteration 200 / 2000: loss 4.845974\n",
      "iteration 300 / 2000: loss 2.264826\n",
      "iteration 400 / 2000: loss 2.202152\n",
      "iteration 500 / 2000: loss 2.129012\n",
      "iteration 600 / 2000: loss 2.164516\n",
      "iteration 700 / 2000: loss 2.168989\n",
      "iteration 800 / 2000: loss 2.135387\n",
      "iteration 900 / 2000: loss 2.204266\n",
      "iteration 1000 / 2000: loss 2.163155\n",
      "iteration 1100 / 2000: loss 2.097850\n",
      "iteration 1200 / 2000: loss 2.228321\n",
      "iteration 1300 / 2000: loss 2.077948\n",
      "iteration 1400 / 2000: loss 2.086478\n",
      "iteration 1500 / 2000: loss 2.158929\n",
      "iteration 1600 / 2000: loss 2.136392\n",
      "iteration 1700 / 2000: loss 2.088451\n",
      "iteration 1800 / 2000: loss 2.218724\n",
      "iteration 1900 / 2000: loss 2.189757\n",
      "That took 7.367252s\n",
      "training accuracy: 0.333449\n",
      "validation accuracy: 0.347000\n",
      "Trying learning_rate:  4e-07  and regularization strength:  43750.0\n",
      "iteration 0 / 2000: loss 1352.659363\n",
      "iteration 100 / 2000: loss 41.264061\n",
      "iteration 200 / 2000: loss 3.286355\n",
      "iteration 300 / 2000: loss 2.195635\n",
      "iteration 400 / 2000: loss 2.122330\n",
      "iteration 500 / 2000: loss 2.094098\n",
      "iteration 600 / 2000: loss 2.230854\n",
      "iteration 700 / 2000: loss 2.188727\n",
      "iteration 800 / 2000: loss 2.206677\n",
      "iteration 900 / 2000: loss 2.250336\n",
      "iteration 1000 / 2000: loss 2.167752\n",
      "iteration 1100 / 2000: loss 2.129795\n",
      "iteration 1200 / 2000: loss 2.180512\n",
      "iteration 1300 / 2000: loss 2.177045\n",
      "iteration 1400 / 2000: loss 2.155058\n",
      "iteration 1500 / 2000: loss 2.195983\n",
      "iteration 1600 / 2000: loss 2.152504\n",
      "iteration 1700 / 2000: loss 2.152830\n",
      "iteration 1800 / 2000: loss 2.147246\n",
      "iteration 1900 / 2000: loss 2.203094\n",
      "That took 7.195631s\n",
      "training accuracy: 0.322653\n",
      "validation accuracy: 0.344000\n",
      "Trying learning_rate:  4e-07  and regularization strength:  50000.0\n",
      "iteration 0 / 2000: loss 1547.295951\n",
      "iteration 100 / 2000: loss 29.064431\n",
      "iteration 200 / 2000: loss 2.673198\n",
      "iteration 300 / 2000: loss 2.204314\n",
      "iteration 400 / 2000: loss 2.167859\n",
      "iteration 500 / 2000: loss 2.149347\n",
      "iteration 600 / 2000: loss 2.196877\n",
      "iteration 700 / 2000: loss 2.148210\n",
      "iteration 800 / 2000: loss 2.163873\n",
      "iteration 900 / 2000: loss 2.133161\n",
      "iteration 1000 / 2000: loss 2.210668\n",
      "iteration 1100 / 2000: loss 2.210860\n",
      "iteration 1200 / 2000: loss 2.229928\n",
      "iteration 1300 / 2000: loss 2.205032\n",
      "iteration 1400 / 2000: loss 2.137097\n",
      "iteration 1500 / 2000: loss 2.192361\n",
      "iteration 1600 / 2000: loss 2.161219\n",
      "iteration 1700 / 2000: loss 2.177556\n",
      "iteration 1800 / 2000: loss 2.157582\n",
      "iteration 1900 / 2000: loss 2.162674\n",
      "That took 7.243383s\n",
      "training accuracy: 0.315755\n",
      "validation accuracy: 0.326000\n",
      "Trying learning_rate:  5e-07  and regularization strength:  25000.0\n",
      "iteration 0 / 2000: loss 783.228210\n",
      "iteration 100 / 2000: loss 64.295327\n",
      "iteration 200 / 2000: loss 7.128367\n",
      "iteration 300 / 2000: loss 2.477444\n",
      "iteration 400 / 2000: loss 2.108196\n",
      "iteration 500 / 2000: loss 2.078778\n",
      "iteration 600 / 2000: loss 2.166511\n",
      "iteration 700 / 2000: loss 2.109830\n",
      "iteration 800 / 2000: loss 2.155970\n",
      "iteration 900 / 2000: loss 2.095367\n",
      "iteration 1000 / 2000: loss 2.072313\n",
      "iteration 1100 / 2000: loss 2.153846\n",
      "iteration 1200 / 2000: loss 2.209415\n",
      "iteration 1300 / 2000: loss 2.064783\n",
      "iteration 1400 / 2000: loss 2.136164\n",
      "iteration 1500 / 2000: loss 2.158318\n",
      "iteration 1600 / 2000: loss 2.110419\n",
      "iteration 1700 / 2000: loss 2.124149\n",
      "iteration 1800 / 2000: loss 2.144662\n",
      "iteration 1900 / 2000: loss 2.067781\n",
      "That took 7.167957s\n",
      "training accuracy: 0.344388\n",
      "validation accuracy: 0.346000\n",
      "Trying learning_rate:  5e-07  and regularization strength:  31250.0\n",
      "iteration 0 / 2000: loss 957.288648\n",
      "iteration 100 / 2000: loss 42.598838\n",
      "iteration 200 / 2000: loss 3.879839\n",
      "iteration 300 / 2000: loss 2.179832\n",
      "iteration 400 / 2000: loss 2.112364\n",
      "iteration 500 / 2000: loss 2.132544\n",
      "iteration 600 / 2000: loss 2.181384\n",
      "iteration 700 / 2000: loss 2.129985\n",
      "iteration 800 / 2000: loss 2.146022\n",
      "iteration 900 / 2000: loss 2.124789\n",
      "iteration 1000 / 2000: loss 2.124059\n",
      "iteration 1100 / 2000: loss 2.181383\n",
      "iteration 1200 / 2000: loss 2.210256\n",
      "iteration 1300 / 2000: loss 2.101443\n",
      "iteration 1400 / 2000: loss 2.160986\n",
      "iteration 1500 / 2000: loss 2.122004\n",
      "iteration 1600 / 2000: loss 2.130736\n",
      "iteration 1700 / 2000: loss 2.166970\n",
      "iteration 1800 / 2000: loss 2.160249\n",
      "iteration 1900 / 2000: loss 2.064383\n",
      "That took 7.048327s\n",
      "training accuracy: 0.338837\n",
      "validation accuracy: 0.349000\n",
      "Trying learning_rate:  5e-07  and regularization strength:  37500.0\n",
      "iteration 0 / 2000: loss 1162.926588\n",
      "iteration 100 / 2000: loss 28.034286\n",
      "iteration 200 / 2000: loss 2.762488\n",
      "iteration 300 / 2000: loss 2.188891\n",
      "iteration 400 / 2000: loss 2.204947\n",
      "iteration 500 / 2000: loss 2.195016\n",
      "iteration 600 / 2000: loss 2.117098\n",
      "iteration 700 / 2000: loss 2.148132\n",
      "iteration 800 / 2000: loss 2.188497\n",
      "iteration 900 / 2000: loss 2.152722\n",
      "iteration 1000 / 2000: loss 2.152402\n",
      "iteration 1100 / 2000: loss 2.165466\n",
      "iteration 1200 / 2000: loss 2.208301\n",
      "iteration 1300 / 2000: loss 2.150310\n",
      "iteration 1400 / 2000: loss 2.169945\n",
      "iteration 1500 / 2000: loss 2.117890\n",
      "iteration 1600 / 2000: loss 2.163745\n",
      "iteration 1700 / 2000: loss 2.190239\n",
      "iteration 1800 / 2000: loss 2.213602\n",
      "iteration 1900 / 2000: loss 2.100982\n",
      "That took 7.170990s\n",
      "training accuracy: 0.337347\n",
      "validation accuracy: 0.350000\n",
      "Trying learning_rate:  5e-07  and regularization strength:  43750.0\n",
      "iteration 0 / 2000: loss 1365.410843\n",
      "iteration 100 / 2000: loss 18.291883\n",
      "iteration 200 / 2000: loss 2.314272\n",
      "iteration 300 / 2000: loss 2.143002\n",
      "iteration 400 / 2000: loss 2.208977\n",
      "iteration 500 / 2000: loss 2.184507\n",
      "iteration 600 / 2000: loss 2.157753\n",
      "iteration 700 / 2000: loss 2.174156\n",
      "iteration 800 / 2000: loss 2.231667\n",
      "iteration 900 / 2000: loss 2.137690\n",
      "iteration 1000 / 2000: loss 2.202190\n",
      "iteration 1100 / 2000: loss 2.110054\n",
      "iteration 1200 / 2000: loss 2.188615\n",
      "iteration 1300 / 2000: loss 2.047826\n",
      "iteration 1400 / 2000: loss 2.179452\n",
      "iteration 1500 / 2000: loss 2.193564\n",
      "iteration 1600 / 2000: loss 2.178271\n",
      "iteration 1700 / 2000: loss 2.135355\n",
      "iteration 1800 / 2000: loss 2.197776\n",
      "iteration 1900 / 2000: loss 2.146092\n",
      "That took 7.014963s\n",
      "training accuracy: 0.332020\n",
      "validation accuracy: 0.347000\n",
      "Trying learning_rate:  5e-07  and regularization strength:  50000.0\n",
      "iteration 0 / 2000: loss 1544.902978\n",
      "iteration 100 / 2000: loss 11.808406\n",
      "iteration 200 / 2000: loss 2.235948\n",
      "iteration 300 / 2000: loss 2.201373\n",
      "iteration 400 / 2000: loss 2.172325\n",
      "iteration 500 / 2000: loss 2.174107\n",
      "iteration 600 / 2000: loss 2.119533\n",
      "iteration 700 / 2000: loss 2.193066\n",
      "iteration 800 / 2000: loss 2.201834\n",
      "iteration 900 / 2000: loss 2.171139\n",
      "iteration 1000 / 2000: loss 2.214129\n",
      "iteration 1100 / 2000: loss 2.184633\n",
      "iteration 1200 / 2000: loss 2.214077\n",
      "iteration 1300 / 2000: loss 2.155781\n",
      "iteration 1400 / 2000: loss 2.192244\n",
      "iteration 1500 / 2000: loss 2.123026\n",
      "iteration 1600 / 2000: loss 2.203435\n",
      "iteration 1700 / 2000: loss 2.209478\n",
      "iteration 1800 / 2000: loss 2.103419\n",
      "iteration 1900 / 2000: loss 2.183231\n",
      "That took 6.953196s\n",
      "training accuracy: 0.330082\n",
      "validation accuracy: 0.343000\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.353531 val accuracy: 0.367000\n",
      "lr 1.000000e-07 reg 3.125000e+04 train accuracy: 0.344551 val accuracy: 0.355000\n",
      "lr 1.000000e-07 reg 3.750000e+04 train accuracy: 0.337041 val accuracy: 0.360000\n",
      "lr 1.000000e-07 reg 4.375000e+04 train accuracy: 0.328939 val accuracy: 0.339000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.332224 val accuracy: 0.342000\n",
      "lr 2.000000e-07 reg 2.500000e+04 train accuracy: 0.346755 val accuracy: 0.369000\n",
      "lr 2.000000e-07 reg 3.125000e+04 train accuracy: 0.343245 val accuracy: 0.361000\n",
      "lr 2.000000e-07 reg 3.750000e+04 train accuracy: 0.340878 val accuracy: 0.351000\n",
      "lr 2.000000e-07 reg 4.375000e+04 train accuracy: 0.326571 val accuracy: 0.340000\n",
      "lr 2.000000e-07 reg 5.000000e+04 train accuracy: 0.327204 val accuracy: 0.338000\n",
      "lr 3.000000e-07 reg 2.500000e+04 train accuracy: 0.353143 val accuracy: 0.351000\n",
      "lr 3.000000e-07 reg 3.125000e+04 train accuracy: 0.340122 val accuracy: 0.355000\n",
      "lr 3.000000e-07 reg 3.750000e+04 train accuracy: 0.342224 val accuracy: 0.348000\n",
      "lr 3.000000e-07 reg 4.375000e+04 train accuracy: 0.328143 val accuracy: 0.337000\n",
      "lr 3.000000e-07 reg 5.000000e+04 train accuracy: 0.327755 val accuracy: 0.340000\n",
      "lr 4.000000e-07 reg 2.500000e+04 train accuracy: 0.348122 val accuracy: 0.364000\n",
      "lr 4.000000e-07 reg 3.125000e+04 train accuracy: 0.332918 val accuracy: 0.339000\n",
      "lr 4.000000e-07 reg 3.750000e+04 train accuracy: 0.333449 val accuracy: 0.347000\n",
      "lr 4.000000e-07 reg 4.375000e+04 train accuracy: 0.322653 val accuracy: 0.344000\n",
      "lr 4.000000e-07 reg 5.000000e+04 train accuracy: 0.315755 val accuracy: 0.326000\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.344388 val accuracy: 0.346000\n",
      "lr 5.000000e-07 reg 3.125000e+04 train accuracy: 0.338837 val accuracy: 0.349000\n",
      "lr 5.000000e-07 reg 3.750000e+04 train accuracy: 0.337347 val accuracy: 0.350000\n",
      "lr 5.000000e-07 reg 4.375000e+04 train accuracy: 0.332020 val accuracy: 0.347000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.330082 val accuracy: 0.343000\n",
      "best validation accuracy achieved during cross-validation: 0.369000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "num_ranges = 5\n",
    "lr_ranges = np.linspace(learning_rates[0],\n",
    "                        learning_rates[1],\n",
    "                        num_ranges)\n",
    "rs_ranges = np.linspace(regularization_strengths[0],\n",
    "                        regularization_strengths[1],\n",
    "                        num_ranges)\n",
    "\n",
    "for lr in lr_ranges:\n",
    "    for rs in rs_ranges:\n",
    "        print('Trying learning_rate: ', lr,\n",
    "              ' and regularization strength: ', rs)\n",
    "\n",
    "        softmax = Softmax()\n",
    "        tic = time.time()\n",
    "        loss_hist = softmax.train(X_train, y_train,\n",
    "                              learning_rate=lr,\n",
    "                              reg=rs,\n",
    "                              num_iters=2000,\n",
    "                              verbose=True)\n",
    "        toc = time.time()\n",
    "        print('That took %fs' % (toc - tic))\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "        train_acc = np.mean(y_train == y_train_pred)\n",
    "\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        val_acc = np.mean(y_val == y_val_pred)\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_softmax = softmax\n",
    "            best_params = {'learning_rate': lr,\n",
    "                           'regularization': rs}\n",
    "        print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))\n",
    "        results.update({(lr, rs): (train_acc, val_acc)})\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.361000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question** - *True or False*\n",
    "\n",
    "It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "*Your answer*:\n",
    "\n",
    "*Your explanation*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5bee45ff98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
